---
title: "TODO sheet for `wordspace` package development"
author: "Stefan Evert"
date: "30 Jun 2019"
output: 
  html_document:
    toc: true
    toc_float: true
---

## Bug fixes and improvements

*  note that (dense) matrix should be saved to text format with `write.table()` rather than `write()` 
    - `write` calls `cat`, inserts a  spurious line break every INT_MAX chars)
    - `write.table` is the proper function to save a matrix in text format anyway and enables us to include variable names
    - options: mention this in examples; write a (new) vignette that shows how to save a file in word2vec format
    - preferred: write simple wrapper function to save matrix in word2vec format (see below)
*  implement `write.dsm.matrix()` for various output formats for dense/sparse matrices?
    - can't be called `write.matrix()`, which already exists in MASS (but doesn't look very optimal)
    - should have options to save row/column names and dense/sparse matrices in various text formats
*  convenience methods for `rownames` and `colnames` of DSM object
    - achieved by providing `dimnames.dsm` method
    - preferably also `dimnames<-` method, which sets `rows$term` and `cols$term` as well as dimnames of `$M` and `$S`
*  `as.matrix.dsm()` method should extract the raw (`$M`) or scored (`$S`) cooccurrence matrix, as appropriate
    - might also update dimnames of the matrix in case someone has messed with `$rows$term` or `$cols$term` directly
    - but explain in manpage that it will often return a sparse matrix
*  `[` accessor method for subsetting with row/column numbers or target/feature terms
*  implement `signcount` in Rcpp for vector / dense matrix (or is `as.vector` sufficient?)
    - counts numbers of negative, zero and positive entries --> efficient `nnzero` and non-negative check for canonical matrices
    - possibly R wrapper function for sparse matrices (dgC, dgT, dgR)
*  make sure there are tests for all important functions
    - also test non-negativity check in `dsm.is.canonical` for sparse and dense matrices


## Combining DSM objects after internal redesign

*  rethink binding and merging
    - `rbind` and `cbind` combine models for different targets with same features, or different features for same targets
    - could be extended to allow for mismatches between columns (`rbind`) or rows (`cbind`); choose between including only common items or all items
    - `merge` is used to pool counts from multiple corpora for the same targets and features
*  `rbind.dsm()` and `merge.dsm()` need to be updated and documented properly
*  implement `cbind.dsm()`
*  implement `all=TRUE` option for `merge.dsm()` ("expanding merge")
*  implement column merge (`rows=FALSE`) for `merge.dsm()`
*  efficient reimplementation of `rbind.dsm()` for sparse matrices (require all arguments to be sparse in normal form; use C code, work with triplet representation, or very clever code on `dgCMatrix` objects)


## Support for evaluation experiments

*  `pair.distances()` should also work with M2 (= M) for cross-distance computations
    - in `pair.distances()`, adjust ranks by one only in non-cross-distance case (`M2=NULL`)
    - currently, adjustment is disabled for pre-computed distance matrix not marked as symmetric
    - **TODO** check whether this is still the case (shouldn't we explicitly check for same label, as `nearest.neighbours` does? but cross-distances are explicitly excluded from the adjustment)
*  also see hand-written notes


## Additional metrics

*  Dice coefficient (Lin 1998)
    - Jaccard metric is already implemented (Dice is not metric), but only for non-negative input
    - is there a meaningful generalization to vectors with negative values?
*  KL divergence and Jensen-Shannon metric (Endres & Schindelin 2003) for non-negative vectors
    - input matrices have to be checked unless they're already marked
    - KL divergence --> skew divergence (Lee 2001)?
    - JS metric --> general Jensen-Shannon divergence with mix parameter \(\lambda\in (0,1)\)?
    - now that we can test for non-negativity, these measures can easily be added
*  generalizations of vector norms/metrics that share aspects of Dice / KL?


## New projection methods

*  implement non-negative version of RI (increases correlations between dimensions adn resulting distortion)
    - what is the effect of distortion from non-orthogonality of dimensions?
    - perhaps this might even be beneficial: is distortion likely to improve similarity of correlated targets?
*  guided random indexing?
    - generate random projections, then preserve the ones with high variance?
    - will probably lead to stronger correlations between dimensions and resulting distortion effects (test this)
    - need to experiment with various settings for random projection density, selection criterion, etc.
*  implement `nndsvd()` according to Boutsidis & Gallopoulos (2008)
    - transforms given SVD into an approximate non-negative decomposition
    - can this be interpreted as a (non-orthogonal) projection? what are the basis vectors?
    - what is a sensible $R^2$ in this case?
    - requires thorough validation test to determine approximation quality and usefulness of this decomposition
*  `dsm.projection()` could have an Option `non.negative=TRUE`
    - only valid if input matrix is non-negative
    - guarantees that projection is also non-negative (by using a non-negative basis with RI, and NNDSVD with SVD)
    - **check status** -- do we have a non-negative version of random indexing?
*  implement NMF (multiplicative updates, cf. Python code)
    - key element: C/C++ function to compute `crossprod()` only for nonzero entries of a given sparse matrix
    - possibly modified version of this function in order to compute approximation error of sparse matrix decomposition
    - NMF can be initialized with `nndsvd()` or NNDSVDar (but only reproducible if the initialization is not obtained by rSVD)
    - alternatively, carry out multiple runs with random initializations and select the best approximation
    - should also experiment with more sophisticated variants of NMF that include regularization terms (--> similar to sparse priors)
    - these might provide a fairly general and flexible projection framework
    - don't forget to check for non-negativity and **mark output as non-negative**
*  experiment with LDA and similar topic models
    - use implementation in `topicmodels` package
    - can this deal with arbitrary non-negative scores, or only (binary?) frequency counts?
    - can this still be interpreted as a projection method, or do we need a second function `dsm.decomp()`?
    - some authors claim that NMF is equivalent to probabilistic LSA, and it may be equivalent to LDA with suitable regularization
*  neural embeddings and sparseness
    - learned neural embeddings (esp. Mikolov's _word2vec_) seem to work extremely well (Baroni, Dinu & Kruszewski 2014)
    - should train word2vec embeddings on various corpora and include as pre-compiled models (Web downloads)
*  preliminary comparison suggests that learned embeddings are more homogeneous
    - column norms of reduced matrix are nearly constant for _word2vec_, whereas first SVD dimensions capture a large part of the variance
    - similarly for Senne embeddings, though with only 50 dims and different training regime, they don't work very well in standard tasks
    - can we generate homogenous embeddings from SVD projection?
*  _word2vec_-like model trained on co-occurrence matrix?
    - what would be an appropriate goal criterion?


## Analysis support
*  implement Chinese Whispers clustering algorithm
    - try different versions: batch update with R code, single updates implemented in C/C++


## Optimization

*  more efficient reading of triplet files
    - `readr` package doesn't seem to help (faster, but even more memory overhead than scan)
    - temporary triplet table requires at least 24 bytes / row, which cannot be avoided
    - main issue is whether a copy of the table has to be made (e.g. because of reallocation while reading input file of unknown size)
    - approach A: if input is file (rather than pipe), count lines first, then scan with `nmax` option; but doesn't seem to help much/systematically in some tests
    - approach B: read input in chunks, then change dsm() constructor so it can work on the chunks and combine them as late as possible into the sparse matrix; it may be difficult to achieve substantial savings with this strategy
*  investigate memory overhead shown by benchmarks
    - cosine distance for dense (and possibly also sparse) matrix; very substantial overhead in "SVD-100 cosine distances" benchmark
        - key problem seems to be copy-on-write duplication of resulting matrix during post-processing, especially when setting attributes
        - not entirely clear when and why this happens; there seem to be at least two extra copies of the matrix despite in-place margin scaling
        - counter-intuitively, without rescaling (`normalized=TRUE`), a third copy seems to be made; whether or not cosine similarities are converted to distances seems to have no effect
        - should use `tracemem()` and `refs()` (from `pryr` package) to investigate
    - dense `dist.matrix()` compared to `dist()`
    - `read.dsm.ucs()` has considerable overhead in R-level implementation of `readMM`
        - triplet representation is read efficiently using `scan` with known size
        - but then returned as a `dgTMatrix`, which is then converted to a `dgCMatrix` by `read.dsm.ucs()`
          - perhaps overhead can be reduced without the intermediate dgTMatrix (but need at least one complete copy for reordering triplets into CSR)
*  `dsm.projection()` could have option `renormalized` that performs row normalization in-place (with additional arguments passed on to rowNorms())


## Miscellaneous

*  do we need to apply `normalizePath()` to input/output filenames?
*  new methods for `as.dsm()`
    - possibly also convenience methods for `matrix` and `dgCMatrix`, which would just be thin wrappers around `dsm()`
*  include basic Shiny GUIs as functions in package?
    - so far: nearest neighbours explorer
    - should try to load shiny package and abort with error if not installed
