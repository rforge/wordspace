# -*-org-*-
#+TITLE: TODO sheet for wordspace package development
#+AUTHOR: Stefan Evert
#+EMAIL:  stefan.evert@fau.de
#+LANGUAGE:  en
#+OPTIONS:   H:3 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="etc/orgmode.css" />

* Urgent updates
    
  - additional documentation
    - write a package vignette based on the COLING demo, but using only data sets included in the package
    - note that (dense) matrix should be saved to text format with =write.table()= rather than =write()= (inserts spurious line break every INT_MAX chars), either in examples or in a (new) vignette
  - upload package to CRAN
    - pre-release version could be made available on the homepage (using WinBuilder)
  - improve Web page


* Bug fixes and improvements

 - include pre-compiled large-corpus DSM for core vocabulary (e.g. from LargeDSM), e.g. basic English + words in included test sets
   - should arrange for easy updates that automatically compile vocabulary from included data sets
   - add further tests
 - special case optimization in =subset.dsm()=: return input unchanged if no rows or columns are dropped (to avoid unnecessary duplication with recursive=TRUE)
 - handle zero rows more gracefully in =normalize.rows()=
   - perhaps leave untouched if row norm is below some =tol= parameter
   - is normalization meaningful for Minkowski norm with $p < 1$, which doesn't scale linearly in our generalization?
 - =normalize=TRUE= in =dsm.score()= might automatically filter out invalid rows (with $\|x\| = 0$) or keep them at 0 (rather than =NA=)
   - perhaps as an option to =normalize.rows()=, but =dsm.score()= would need to handle this specially and drop deleted rows from entire model
 - more efficient reading of triplet files
   - =readr= package doesn't seem to help (faster, but even more memory overhead than scan)
   - temporary triplet table requires at least 24 bytes / row, which cannot be avoided
   - main issue is whether a copy of the table has to be made (e.g. because of reallocation while reading input file of unknown size)
   - approach A: if input is file (rather than pipe), count lines first, then scan with =nmax= option; but doesn't seem to help much/systematically in some tests
   - approach B: read input in chunks, then change dsm() constructor so it can work on the chunks and combine them as late as possible into the sparse matrix; it may be difficult to achieve substantial savings with this strategy


* Combining DSM objects after internal redesign

  - rethink binding and merging
    - =rbind= and =cbind= combine models for different targets with same features, or different features for same targets
    - could be extended to allow for mismatches between columns (=rbind=) or rows (=cbind=); choose between including only common items or all items
    - =merge= is used to pool counts from multiple corpora for the same targets and features
  - =rbind.dsm()= and =merge.dsm()= need to be updated and documented properly
  - implement =cbind.dsm()=
  - implement =all=TRUE= option for =merge.dsm()= ("expanding merge")
  - implement column merge (=rows=FALSE=) for =merge.dsm()=
  - efficient reimplementation of =rbind.dsm()= for sparse matrices (require all arguments to be sparse in normal form; use C code, work with triplet representation, or very clever code on =dgCMatrix= objects)


* Non-negative matrices and projections

  - Matrizen werden mit Attribut =non.negative= versehen; Frequenzmatrix M muss nonneg sein (wird von Konstruktor überprüft); S wird von =dsm.score()= markiert, falls sichergestellt ist, dass die Matrix keine negativen Elemente enthalten kann (=negative.ok=FALSE=)
  - diverse Funktionen wie =dsm.score()=, =dsm.projection()=, =dist.matrix()=, ...  erlauben bestimmte Optionen / Algorithmen nur für nonneg-Matrizen; diese werden am Attribut =non.negative= erkannt; wenn Attribut fehlt, wird die Eingabematrix geprüft; evtl. expliziter Override mit =non.negative=TRUE= (aber sinnvoll?)
  - dafür C-Funktion, um Vektor/Matrix speichereffizient auf nonneg zu testen; in Methode =is.nonnegative()= einbinden, mit Methoden für numeric, matrix, und Matrix (csr oder triplet)
  - Ausgabe von dsm.projection mit NMF oder anderem nonnoeg-Algorithmus wird ebenfalls mit Attribut =non.negative=TRUE= markiert


* Support for evaluation experiments

  - =as.dist.matrix(M, sim=FALSE)= to mark as distance matrix (so =nearest.neighbours=, =pair.distances=, etc. can be used)
    - methods for dense and sparse canonical matrix (perhaps as =as.dist.matrix.default=)
    - method for =dist= objects, which converts them to full symmetric matrix
  - =pair.distances()= should also work with M2 (= M) for cross-distance computations
    - in =pair.distances()=, adjust ranks by one only in non-cross-distance case (=M2=NULL=)
  - =pair.distances()= should accept pre-computed =dist.matrix= (both distance and similarity scores, marked appropriately)
    - assumed to be cross-distance case if distance matrix is not marked symmetric
    - can be used to safely read out association scores from "fake" distance matrix (marked with =as.dist.matrix=)
  - also see hand-written notes


* Additional metrics

  - Dice and/or Jaccard coefficient (Lin 1998)
    - is there a meaningful formulation for non-negative vectors?
  - KL divergence and Jensen-Shannon metric (Endres & Schindelin 2003) for non-negative vectors
    - input matrices have to be checked unless they're already marked
    - KL divergence --> skew divergence (Lee 2001)?
    - JS metric --> general Jensen-Shannon divergence with mix parameter \(\lambda\in (0,1)\)?
  - generalizations of vector norms/metrics that share aspects of Dice / KL?


* New projection methods

  - implement non-negative version of RI (increases correlations between dimensions adn resulting distortion)
    - what is the effect of distortion from non-orthogonality of dimensions?
    - perhaps this might even be beneficial: is distortion likely to improve similarity of correlated targets?
  - guided random indexing?
    - generate random projections, then preserve the ones with high variance?
    - will probably lead to stronger correlations between dimensions and resulting distortion effects (test this)
    - need to experiment with various settings for random projection density, selection criterion, etc.
  - implement =nndsvd()= according to Boutsidis & Gallopoulos (2008)
    - transforms given SVD into an approximate non-negative decomposition
    - can this be interpreted as a (non-orthogonal) projection? what are the basis vectors?
    - what is a sensible $R^2$ in this case?
    - requires thorough validation test to determine approximation quality and usefulness of this decomposition
  - =dsm.projection()= could have an Option =non.negative=TRUE=
    - only valid if input matrix is non-negative
    - guarantees that projection is also non-negative (by using a non-negative basis with RI, and NNDSVD with SVD)
  - implement NMF (multiplicative updates, cf. Python code)
    - key element: C/C++ function to compute =crossprod()= only for nonzero entries of a given sparse matrix
    - possibly modified version of this function in order to compute approximation error of sparse matrix decomposition
    - NMF can be initialized with =nndsvd()= or NNDSVDar (but only reproducible if the initialization is not obtained by rSVD)
    - alternatively, carry out multiple runs with random initializations and select the best approximation
    - should also experiment with more sophisticated variants of NMF that include regularization terms (--> similar to sparse priors)
      - these might provide a fairly general and flexible projection framework
  - experiment with LDA and similar topic models
    - use implementation in =topicmodels= package
    - can this deal with arbitrary non-negative scores, or only (binary?) frequency counts?
    - can this still be interpreted as a projection method, or do we need a second function =dsm.decomp()=?
    - some authors claim that NMF is equivalent to probabilistic LSA, and it may be equivalent to LDA with suitable regularization
  - neural embeddings and sparseness
    - learned neural embeddings (esp. Mikolov's /word2vec/) seem to work extremely well (Baroni, Dinu & Kruszewski 2014)
    - should train word2vec embeddings on various corpora and include as pre-compiled models (Web downloads)
  - preliminary comparison suggests that learned embeddings are more homogeneous
    - column norms of reduced matrix are nearly constant for /word2vec/, whereas first SVD dimensions capture a large part of the variance
    - similarly for Senne embeddings, though with only 50 dims and different training regime, they don't work very well in standard tasks
    - can we generate homogenous embeddings from SVD projection?
  - /word2vec/-like model trained on co-occurrence matrix?
    - what would be an appropriate goal criterion?


* Analysis support
  - implement Chinese Whispers clustering algorithm
    - try different versions: batch update with R code, single updates implemented in C/C++


* Optimization

  - investigate memory overhead shown by benchmarks
    - cosine distance for dense (and possibly also sparse) matrix; very substantial overhead in "SVD-100 cosine distances" benchmark
    - dense dist.matrix() compared to dist()
  - =dsm.projection()= could have option =renormalized= that performs row normalization in-place (with additional arguments passed on to rowNorms())


* Miscellaneous

  - do we need to apply =normalizePath()= to input/output filenames?
  - new methods for =as.dsm()=
    - possibly also convenience methods for =matrix= and =dgCMatrix=, which would just be thin wrappers around =dsm()=
  - include basic Shiny GUIs as functions in package?
    - so far: nearest neighbours explorer
    - should try to load shiny package and abort with error if not installed
  - implement =write.dsm.matrix()= for various output formats for dense/sparse matrices?
    - can't be called =write.matrix()=, which already exists in MASS (but doesn't look very optimal)
    - should have options to save row/column names and dense/sparse matrices in various text formats
