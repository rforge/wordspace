%% \documentclass[handout,t]{beamer} % HANDOUT
%% \documentclass[handout,notes=show,t]{beamer} % NOTES
\documentclass[t]{beamer} % SLIDES
\usepackage{etex}

\usetheme{DSM}
\usepackage{beamer-tools-dsm}

\input{lib/math}  % basic mathematical notation
\input{lib/text}  % some useful macros for plain text
\input{lib/stat}  % notation for probability theory and statistics
\input{lib/vector}% convenience macros for vectors and matrices

\input{local/config} % local adjustments to configuration and macros

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Titlepage

\title[DSM Tutorial -- Part 1]{Distributional Semantic Models}
\subtitle{Part 1: Introduction}
\author[\textcopyright\ Evert/Lenci/Baroni/Lapesa]{%
  Stefan Evert\inst{1}\\
  {\footnotesize with  Alessandro Lenci\inst{2}, Marco Baroni\inst{3} and Gabriella Lapesa\inst{4}}}
\institute[CC-by-sa]{%
  \inst{1}Friedrich-Alexander-Universit채t Erlangen-N체rnberg, Germany\\
  \inst{2}University of Pisa, Italy\\
  \inst{3}University of Trento, Italy\\
  \inst{4}University of Stuttgart, Germany
}

\date[wordspace.collocations.de]{
  \href{http://wordspace.collocations.de/doku.php/course:start}{\primary{\small http://wordspace.collocations.de/doku.php/course:start}}\\
  \light{\tiny \dsmcopyright}}

\begin{document}

\showLogo
\frame{\titlepage}
\hideLogo

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Outline}
\frame{ 
  \frametitle{Outline}
  \tableofcontents
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The distributional hypothesis}

\begin{frame}[c]
  \frametitle{Meaning \& distribution}
  % \framesubtitle{}

  \begin{itemize}
  \item ``Die Bedeutung eines Wortes liegt in seinem Gebrauch.''\\
    \hfill --- Ludwig Wittgenstein
  \item[]
  \item ``You shall know a word by the company it keeps!''\\
    \hfill --- J.~R.\ \citet{Firth:57}
  \item[]
  \item Distributional hypothesis: difference of meaning correlates with difference of distribution \citep[Zellig][]{Harris:54}
  \item[]
  \item ``What people know when they say that they know a word is not how to recite its dictionary definition -- they know how to use it [\ldots] in everyday discourse.'' \citep{Miller:86}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{What is the meaning of ``\textbf{bardiwac}''?}
  % \framesubtitle{}

  \begin{itemize}
  \item<2-> He handed her her glass of \primary{bardiwac}.
  \item<3-> Beef dishes are made to complement the \primary{bardiwacs}.
  \item<4-> Nigel staggered to his feet, face flushed from too much \primary{bardiwac}.
  \item<5-> Malbec, one of the lesser-known \primary{bardiwac} grapes, responds well to Australia's sunshine.
  \item<6-> I dined off bread and cheese and this excellent \primary{bardiwac}.
  \item<7-> The drinks were delicious: blood-red \primary{bardiwac} as well as light, sweet Rhenish.
  \item[\hand]<8-> bardiwac is a heavy red alcoholic beverage made from grapes
  \end{itemize}

  \gap
  \begin{small}
    \visible<8->{\light{The examples above are handpicked, of course.  But in a corpus like the BNC, you will find at least as many informative sentences.}}
  \end{small}
\end{frame}

\begin{frame}[c]
  \frametitle{What is the meaning of ``\textbf{bardiwac}''?}

  \centering
  \only<beamer:1| handout:0>{\includegraphics[width=10cm]{img/SE_bardiwac_conc}}%
  \only<beamer:2| handout:1>{\includegraphics[width=10cm]{img/SE_bardiwac_sketch}}%
\end{frame}

{\newcommand{\hg}[1]{\scriptsize\textpmhg{#1}}
\newcommand<>{\colA}[1]{\purered#2{#1}}
\newcommand<>{\colB}[1]{\puregreen#2{\textbf#2{#1}}}
\begin{frame}<beamer:1-4| handout:1-4>
  \frametitle{A thought experiment: deciphering hieroglyphs}
  % \framesubtitle{}

  \begin{center}
    \setlength{\arrayrulewidth}{1pt}
    \begin{tabular}{@{\rule{0mm}{1.2em} }lr*{6}{|c}|}
      && \hg{get} & \hg{sij} & \hg{ius} & \hg{hir} & \hg{iit} & \hg{kil} \\
      \hline
      \colB<beamer:2| handout:2>{(knife)} & \colB<beamer:2| handout:2>{\hg{naif}} & \colB<beamer:2| handout:2>{51} & \colB<beamer:2| handout:2>{20} & \colB<beamer:2| handout:2>{84} &  \colB<beamer:2| handout:2>{0} &  \colB<beamer:2| handout:2>{3} &  \colB<beamer:2| handout:2>{0} \\
      \hline
      \colB<beamer:4| handout:4>{(cat)}   & \colB<beamer:4| handout:4>{\hg{ket}}  &  \colB<beamer:4| handout:4>{52} & \colB<beamer:4| handout:4>{58} &  \colB<beamer:4| handout:4>{4} &  \colB<beamer:4| handout:4>{4} &  \colB<beamer:4| handout:4>{6} & \colB<beamer:4| handout:4>{26} \\
      \hline
      \h{???} & \h{\hg{dog}} & \colA{115} & \colA{83} & \colA{10} & \colA{42} & \colA{33} & \colA{17} \\
      \hline
      (boat)  & \hg{beut} &  59 & 39 & 23 &  4 &  0 &  0 \\
      \hline
      (cup)   & \hg{kap}  &  98 & 14 &  6 &  2 &  1 &  0 \\
      \hline
      \colB<beamer:3| handout:3>{(pig)}  & \colB<beamer:3| handout:3>{\hg{pigij}} &  \colB<beamer:3| handout:3>{12} & \colB<beamer:3| handout:3>{17} &  \colB<beamer:3| handout:3>{3} &  \colB<beamer:3| handout:3>{2} &  \colB<beamer:3| handout:3>{9} & \colB<beamer:3| handout:3>{27} \\
      \hline
      (banana) & \hg{nana} & 11 &  2 &  2 &  0 & 18 &  0 \\
      \hline
    \end{tabular}
    
    \gap[2]\Large
    \only<beamer:2| handout:2>{%
      sim(\colA{\hg{dog}}, \colB{\hg{naif}}) = 0.770 }%
    \only<beamer:3| handout:3>{%
      sim(\colA{\hg{dog}}, \colB{\hg{pigij}}) = 0.939 }%
    \only<beamer:4| handout:4>{%
      sim(\colA{\hg{dog}}, \colB{\hg{ket}}) = 0.961 }%
  \end{center}

  \addnote{Similarity scores are cosine similarities on sparse log-scaled frequencies ($\log (f+1)$).}%
\end{frame}

\begin{frame}
  \frametitle{English as seen by the computer \ldots}
  % \framesubtitle{}

  \begin{center}
    \ungap[1]
    \setlength{\arrayrulewidth}{1pt}
    \begin{tabular}{@{\rule{0mm}{1.2em} }l@{ }r*{6}{|c}|}
      && get & see & use & hear & eat & kill \\
      && \hg{get} & \hg{sij} & \hg{ius} & \hg{hir} & \hg{iit} & \hg{kil} \\
      \hline
      knife & \hg{naif} &  51 & 20 & 84 &  0 &  3 &  0 \\
      \hline
      cat   & \hg{ket}  &  52 & 58 &  4 &  4 &  6 & 26 \\
      \hline
      \h{dog} & \h{\hg{dog}} & \colA{115} & \colA{83} & \colA{10} & \colA{42} & \colA{33} & \colA{17} \\
      \hline
      boat  & \hg{beut} &  59 & 39 & 23 &  4 &  0 &  0 \\
      \hline
      cup   & \hg{kap}  &  98 & 14 &  6 &  2 &  1 &  0 \\
      \hline
      pig  & \hg{pigij} &  12 & 17 &  3 &  2 &  9 & 27 \\
      \hline
      banana & \hg{nana} & 11 &  2 &  2 &  0 & 18 &  0 \\
      \hline
    \end{tabular}
  \end{center}
  \hfill\light{\footnotesize verb-object counts from British National Corpus}
\end{frame}
}

\begin{frame}
  \frametitle{Geometric interpretation}
  % \framesubtitle{}

  \begin{columns}[T]
    \begin{column}{40mm}
      \begin{itemize}
      \item row vector \primary{$\vx_{\text{dog}}$} describes usage of word \emph{dog} in the corpus
      \item can be seen as coordinates of point in $n$-dimensional Euclidean space
       \end{itemize}
    \end{column}
    \begin{column}{75mm}      
      \gap[2]
      \begin{small}
        \setlength{\arrayrulewidth}{1pt}
        \begin{tabular}{r*{6}{|c}|}
          & get & see & use & hear & eat & kill \\
          \hline
          knife &  51 & 20 & 84 &  0 &  3 &  0 \\
          \hline
          cat  &  52 & 58 &  4 &  4 &  6 & 26 \\
          \hline
          \h{dog} & \primary{115} & \primary{83} & \primary{10} & \primary{42} & \primary{33} & \primary{17} \\
          \hline
          boat &  59 & 39 & 23 &  4 &  0 &  0 \\
          \hline
          cup  &  98 & 14 &  6 &  2 &  1 &  0 \\
          \hline
          pig  &  12 & 17 &  3 &  2 &  9 & 27 \\
          \hline
          banana & 11 &  2 &  2 &  0 & 18 &  0 \\
          \hline
        \end{tabular}
      \end{small}

      \begin{center}
        \h{co-occurrence matrix} $\mathbf{M}$
      \end{center}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Geometric interpretation}
  % \framesubtitle{}

  \begin{columns}[T]
    \begin{column}{40mm}
      \begin{itemize}
      \item row vector \primary{$\vx_{\text{dog}}$} describes usage of word \emph{dog} in the corpus
      \item can be seen as coordinates of point in $n$-dimensional Euclidean space
      \item illustrated for two dimensions:\\ \emph{get} and \emph{use}
      \item \primary{$\vx_{\text{dog}} = (115,10)$}
      \end{itemize}
    \end{column}
    \begin{column}{75mm}      
      \ungap[1]
      \includegraphics[width=75mm]{img/hieroglyph_2d_1}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Geometric interpretation}
  % \framesubtitle{}

  \begin{columns}[T]
    \begin{column}{40mm}
      \begin{itemize}
      \item similarity = spatial proximity (Euclidean dist.)
      \item location depends on frequency of noun ($f_{\text{dog}} \approx 2.7\cdot f_{\text{cat}}$)
      \end{itemize}
    \end{column}
    \begin{column}{75mm}      
      \ungap[1]
      \includegraphics[width=75mm]{img/hieroglyph_2d_2}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Geometric interpretation}
  % \framesubtitle{}

  \begin{columns}[T]
    \begin{column}{40mm}
      \begin{itemize}
      \item similarity = spatial proximity (Euclidean dist.)
      \item location depends on frequency of noun ($f_{\text{dog}} \approx 2.7\cdot f_{\text{cat}}$)
      \item direction more important than location
      \end{itemize}
    \end{column}
    \begin{column}{75mm}      
      \ungap[1]
      \includegraphics[width=75mm]{img/hieroglyph_2d_3}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Geometric interpretation}
  % \framesubtitle{}

  \begin{columns}[T]
    \begin{column}{40mm}
      \begin{itemize}
      \item similarity = spatial proximity (Euclidean dist.)
      \item location depends on frequency of noun ($f_{\text{dog}} \approx 2.7\cdot f_{\text{cat}}$)
      \item direction more important than location
      \item<1-> normalise ``length'' $\norm{\vx_{\text{dog}}}$ of vector
      \item<2-> or use angle $\alpha$ as distance measure
      \end{itemize}
    \end{column}
    \begin{column}{75mm}
      \ungap[1]
      \only<beamer:1| handout:0>{%
        \includegraphics[width=75mm]{img/hieroglyph_2d_4}}%
      \only<beamer:2| handout:1>{%
        \includegraphics[width=75mm]{img/hieroglyph_2d_5}}%
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Semantic distances}
  % \framesubtitle{}

  \begin{columns}[T]
    \begin{column}{55mm}
      \begin{itemize}
      \item main result of distributional analysis are ``semantic'' distances between words
      \item typical applications
        \begin{itemize}
        \item nearest neighbours
        \item clustering of related words
        \item construct semantic map
        \end{itemize}
      \item other applications require clever use of the distance information
        \begin{itemize}
        \item semantic relations
        \item relational analogies
        \item word sense disambiguation
        \item detection of multiword expressions
        \end{itemize}
      \end{itemize}
    \end{column}
    \begin{column}{55mm}
      \ungap[1]
      \includegraphics[width=50mm]{img/hieroglyph_clustering}

      \gap[1]
      \includegraphics[width=50mm]{img/hieroglyph_semantic_map}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Some applications in computational linguistics}
  % \framesubtitle{}

  \ungap
  \begin{itemize}
  \item Unsupervised part-of-speech induction \citep{Schuetze:95}
  \item Word sense disambiguation \citep{Schuetze:98}
  \item Query expansion in information retrieval \citep{Grefenstette:94}
  \item Synonym tasks \& other language tests\\\ \citep{Landauer:Dumais:97,Turney:etc:03}
  \item Thesaurus compilation \citep{Lin:98b,Rapp:04a}
  \item Ontology \& wordnet expansion \citep{Pantel:etc:09}
  \item Attachment disambiguation \citep*{Pantel:Lin:00}
  \item Probabilistic language models \citep{Bengio:etc:03b}
  \item Subsymbolic input representation for neural networks
  \item Many other tasks in computational semantics:\\
    entailment detection, noun compound interpretation, identification
    of noncompositional expressions, \ldots
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Three famous examples}

\begin{frame}
  \frametitle{Latent Semantic Analysis \citep{Landauer:Dumais:97}}
  % \framesubtitle{}

  \begin{itemize}
  \item Corpus: 30,473 articles from Grolier's \emph{Academic American Encyclopedia} (4.6 million words in total)
    \begin{itemize}
    \item[\hand] articles were limited to first 2,000 characters
    \end{itemize}
  \item Word-article frequency matrix for 60,768 words
    \begin{itemize}
    \item row vector shows frequency of word in each article
    \end{itemize}
  \item Logarithmic frequencies scaled by word entropy
  \item Reduced to 300 dim.\ by singular value decomposition (SVD)
    \begin{itemize}
    \item borrowed from LSI \citep{Dumais:etc:88}
    \item[\hand] central claim: SVD reveals latent semantic features,\\
      not just a data reduction technique
    \end{itemize}
  \item Evaluated on TOEFL synonym test (80 items)
    \begin{itemize}
    \item LSA model achieved 64.4\% correct answers
    \item also simulation of learning rate based on TOEFL results
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Word Space \citep{Schuetze:92,Schuetze:93,Schuetze:98}}
  % \framesubtitle{}

  \begin{itemize}
  \item Corpus: $\approx 60$ million words of news messages
    \begin{itemize}
    \item from the \emph{New York Times} News Service
    \end{itemize}
  \item Word-word co-occurrence matrix
    \begin{itemize}
    \item 20,000 target words \& 2,000 context words as features
    \item row vector records how often each context word occurs close
      to the target word (co-occurrence)
    \item co-occurrence window: left/right 50 words \citep{Schuetze:98}\\
      or $\approx 1000$ characters \citep{Schuetze:92}
    \end{itemize}
  \item Rows weighted by inverse document frequency (tf.idf)
  \item Context vector = centroid of word vectors (bag-of-words)
    \begin{itemize}
    \item[\hand] goal: determine ``meaning'' of a context
    \end{itemize}
  \item Reduced to 100 SVD dimensions (mainly for efficiency)
  \item Evaluated on unsupervised word sense induction by clustering
    of context vectors (for an ambiguous word)
    \begin{itemize}
    \item induced word senses improve information retrieval performance
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{HAL \citep{Lund:Burgess:96}}
  % \framesubtitle{}

  \begin{itemize}
  \item HAL = Hyperspace Analogue to Language
  \item Corpus: 160 million words from newsgroup postings
  \item Word-word co-occurrence matrix
    \begin{itemize}
    \item same 70,000 words used as targets and features
    \item co-occurrence window of 1 -- 10 words
    \end{itemize}
  \item Separate counts for left and right co-occurrence
    \begin{itemize}
    \item i.e.\ the context is \emph{structured}
    \end{itemize}
  \item In later work, co-occurrences are weighted by (inverse) distance \citep{Li:Burgess:Lund:00}
  \item Applications include construction of semantic vocabulary maps
    by multidimensional scaling to 2 dimensions
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Many parameters \ldots}
  % \framesubtitle{}

  \begin{itemize}
  \item Enormous range of DSM parameters and applications
  \item Examples showed three entirely different models, each tuned to
    its particular application
  \item[\So] Need overview of DSM parameters \& understand their effects
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Distributional semantic models}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Definition \& overview}

\begin{frame}
  \frametitle{General definition of DSMs}
  % \framesubtitle{}

  \ungap
  \begin{block}{}
    A \h{distributional semantic model} (DSM) is a scaled and/or
    transformed co-occurrence matrix $\mathbf{M}$, such that each row $\vx$
    represents the distribution of a target term across contexts.
  \end{block}

  \begin{center}
    \begin{small}
      \setlength{\arrayrulewidth}{1pt}
      \begin{tabular}{r*{6}{|c}|}
        & get & see & use & hear & eat & kill \\
        \hline
        knife &  0.027 & -0.024 &  0.206 & -0.022 & -0.044 & -0.042 \\
        \hline
        cat   &  0.031 &  0.143 & -0.243 & -0.015 & -0.009 &  0.131 \\
        \hline
        \primary{dog}   & \primary{-0.026} &  \primary{0.021} & \primary{-0.212} &  \primary{0.064} &  \primary{0.013} &  \primary{0.014} \\
        \hline
        boat  & -0.022 &  0.009 & -0.044 & -0.040 & -0.074 & -0.042 \\
        \hline
        cup   & -0.014 & -0.173 & -0.249 & -0.099 & -0.119 & -0.042 \\
        \hline
        pig   & -0.069 &  0.094 & -0.158 &  0.000 &  0.094 &  0.265 \\
        \hline
        banana&  0.047 & -0.139 & -0.104 & -0.022 &  0.267 & -0.042 \\
        \hline
      \end{tabular}
    \end{small}
  \end{center}

  \hh{Term} = word, lemma, phrase, morpheme, word pair, \ldots
\end{frame}

\begin{frame}
  \frametitle{General definition of DSMs}
  % \framesubtitle{}

  Mathematical notation:
  \begin{itemize}
  \item $k \times n$ co-occurrence matrix $\mathbf{M}$ (example: $7\times 6$ matrix)
    \begin{itemize}
    \item $k$ rows = target terms
    \item $n$ columns = features or \hh{dimensions}
    \end{itemize}
    \begin{small}
      \gap[.5]
      \[
      \mathbf{M} =
      \begin{bmatrix}
        m_{11} & m_{12} & \cdots & m_{1n} \\
        m_{21} & m_{22} & \cdots & m_{2n} \\
        \vdots & \vdots & & \vdots \\
        m_{k1} & m_{k2} & \cdots & m_{kn}
      \end{bmatrix}
      \]
    \end{small}
  \item distribution vector $\Vector{m}_i$ = $i$-th row of $\mathbf{M}$, e.g.\ $\Vector{m}_3 = \Vector{m}_{\text{dog}}$
  \item components $\Vector{m}_i = (m_{i1}, m_{i2}, \ldots, m_{in})$ = features of $i$-th term:
    \begin{align*}
      \Vector{m}_3 &= (-0.026, 0.021, -0.212, 0.064, 0.013, 0.014) \\
      &= (m_{31}, m_{32}, m_{33}, m_{34}, m_{35}, m_{36})
    \end{align*}
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Overview of DSM parameters}
  % \framesubtitle{}

  \ungap[1]
  \begin{center}
    Term-context \vs\ term-term matrix\\
    \pause $\Downarrow$\\
    Definition of terms \& linguistic pre-processing\\
    \pause $\Downarrow$\\
    Size \& type of context\\
    \pause $\Downarrow$\\
    Geometric \vs\ probabilistic interpretation\\
    \pause $\Downarrow$\\
    Feature scaling\\
    \pause $\Downarrow$\\
    Normalisation of rows and/or columns\\
    \pause $\Downarrow$\\
    Similarity / distance measure\\
    \pause $\Downarrow$\\
    Dimensionality reduction
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Term-context matrix}
  % \framesubtitle{}

  \h{Term-context matrix} records frequency of term in each individual context (e.g.\ sentence, document, Web page, encyclopaedia article)
  
  \gap[3]
  \begin{center}
  \(
  \mathbf{F} = 
  \begin{bmatrix}
    \cdots & \vf_1 & \cdots \\
    \cdots & \vf_2 & \cdots \\
    & \vdots & \\
    & \vdots & \\
    \cdots & \vf_k & \cdots \\
  \end{bmatrix}
  \)
  \hspace{5mm}
  \begin{small}
    \setlength{\arrayrulewidth}{1pt}
    \begin{tabular}[c]{r*{7}{|c}|}
      \multicolumn{1}{c}{}
      & \multicolumn{1}{c}{\rotLabel{Felidae}}
      & \multicolumn{1}{c}{\rotLabel{Pet}}
      & \multicolumn{1}{c}{\rotLabel{Feral}}
      & \multicolumn{1}{c}{\rotLabel{Bloat}}
      & \multicolumn{1}{c}{\rotLabel{Philosophy}}
      & \multicolumn{1}{c}{\rotLabel{Kant}}
      & \multicolumn{1}{c}{\rotLabel{Back pain}} \\
      \cline{2-8}
      cat      &      10 &  10 &     7 &    -- &         -- &   -- &        -- \\
      \cline{2-8}
      dog      &      -- &  10 &     4 &    11 &         -- &   -- &        -- \\
      \cline{2-8}
      animal   &       2 &  15 &    10 &     2 &         -- &   -- &        -- \\
      \cline{2-8}
      time     &       1 &  -- &    -- &    -- &          2 &    1 &        -- \\
      \cline{2-8}
      reason   &      -- &   1 &    -- &    -- &          1 &    4 &         1 \\
      \cline{2-8}
      cause    &      -- &  -- &    -- &     2 &          1 &    2 &         6 \\
      \cline{2-8}
      effect   &      -- &  -- &    -- &     1 &         -- &    1 &        -- \\
      \cline{2-8}
    \end{tabular}
  \end{small}
  \end{center}

\end{frame}

\begin{frame}
  \frametitle{Term-context matrix}
  % \Framesubtitle{}

  Some footnotes:
  \begin{itemize}
  \item Features are usually context \secondary{tokens}, i.e.\ individual instances
  \item Can also be generalised to context \secondary{types}, e.g.
    \begin{itemize}
    \item bag of content words
    \item specific pattern of POS tags 
    \item n-gram of words (or POS tags) around target
    \item subcategorisation pattern of target verb
    \end{itemize}
  \item Term-context matrix is often very \h{sparse}
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Term-term matrix}
  % \framesubtitle{}

  \h{Term-term matrix} records co-occurrence frequencies with feature terms for each target term 

  \gap[2]
  \begin{center}
  \(
  \mathbf{M} = 
  \begin{bmatrix}
    \cdots & \vm_1 & \cdots \\
    \cdots & \vm_2 & \cdots \\
    & \vdots & \\
    & \vdots & \\
    \cdots & \vm_k & \cdots \\
  \end{bmatrix}
  \)
  \hspace{3mm}
  \begin{small}
    \setlength{\arrayrulewidth}{1pt}
    \begin{tabular}[c]{r|c@{$\;$}*{6}{|@{$\;$}c@{$\;$}}|}
      \multicolumn{1}{c}{}
      & \multicolumn{1}{c}{\rotLabel{breed}}
      & \multicolumn{1}{c}{\rotLabel{tail}}
      & \multicolumn{1}{c}{\rotLabel{feed}}
      & \multicolumn{1}{c}{\rotLabel{kill}}
      & \multicolumn{1}{c}{\rotLabel{important}}
      & \multicolumn{1}{c}{\rotLabel{explain}}
      & \multicolumn{1}{c}{\rotLabel{likely}} \\
      \cline{2-8}
      cat     &  83 &  17 &   7 &  37 &  -- &   1 &  -- \\
      \cline{2-8}
      dog     & 561 &  13 &  30 &  60 &   1 &   2 &   4 \\
      \cline{2-8}
      animal  &  42 &  10 & 109 & 134 &  13 &   5 &   5 \\
      \cline{2-8}
      time    &  19 &   9 &  29 & 117 &  81 &  34 & 109 \\
      \cline{2-8}
      reason  &   1 &  -- &   2 &  14 &  68 & 140 &  47 \\
      \cline{2-8}
      cause   &  -- &   1 &  -- &   4 &  55 &  34 &  55 \\
      \cline{2-8}
      effect  &  -- &  -- &   1 &   6 &  60 &  35 &  17 \\
      \cline{2-8}
    \end{tabular}
  \end{small}
  \end{center}

  \begin{itemize}
  \item[\hand] we will usually assume a term-term matrix in this tutorial
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Term-term matrix}
  % \Framesubtitle{}

  Some footnotes:
  \begin{itemize}
  \item Often target terms $\neq$ feature terms
    \begin{itemize}
    \item e.g.\ nouns described by co-occurrences with verbs as features
    \item identical sets of target \& feature terms \so symmetric matrix
    \end{itemize}
  \item Different types of contexts \citep{Evert:08}
    \begin{itemize}
    \item \hh{surface context} (word or character window)
    \item \hh{textual context} (non-overlapping segments)
    \item \hh{syntactic contxt} (specific syntagmatic relation)
    \end{itemize}
  \item Can be seen as smoothing of term-context matrix
    \begin{itemize}
    \item average over similar contexts (with same context terms)
    \item data sparseness reduced, except for small windows
    \item we will take a closer look at the relation between term-context and term-term models later in this tutorial
    \end{itemize}
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Using DSM distances}

\begin{frame}
  \frametitle{Nearest neighbours}
  \framesubtitle{DSM based on verb-object relations from BNC, reduced to 100 dim.\ with SVD}
  % \framesubtitle{}

  Neighbours of \h{dog} (cosine angle):
  \begin{itemize}\item[\hand]
    girl (45.5), boy (46.7), horse(47.0), wife (48.8), baby (51.9),
    daughter (53.1), side (54.9), mother (55.6), boat (55.7),
    rest (56.3), night (56.7), cat (56.8), son (57.0), man (58.2), 
    place (58.4), husband (58.5), thing (58.8), friend (59.6), \ldots
  \end{itemize}

  \gap
  Neighbours of \h{school}:
  \begin{itemize}\item[\hand]
    country (49.3), church (52.1), hospital (53.1), house (54.4),
    hotel (55.1), industry (57.0), company (57.0), home (57.7), family
    (58.4), university (59.0), party (59.4), group (59.5), building
    (59.8), market (60.3), bank (60.4), business (60.9), area (61.4),
    department (61.6), club (62.7), town (63.3), library (63.3), 
    room (63.6), service (64.4), police (64.7), \ldots
  \end{itemize}
  \addnote{Neighbours and neighbourhood plots from BNC verb-object DSM, reduced to 100 dimensions by SVD.}%
\end{frame}

\begin{frame}[c]
  \frametitle{Nearest neighbours}
  % \framesubtitle{}

  \ungap[1]
  \begin{center}
    \includegraphics[width=8cm]{img/neighbourhood_dog}
  \end{center}
\end{frame}

\begin{frame}[c]
  \frametitle{Semantic maps}
  % \framesubtitle{}

  \begin{center}
    \includegraphics[width=100mm]{img/hieroglyph_semantic_map}
  \end{center}
\end{frame}

\begin{frame}[c]
  \frametitle{Clustering}
  % \framesubtitle{}

  \begin{center}
    \includegraphics[width=100mm]{img/hieroglyph_clustering}
  \end{center}
\end{frame}

\begin{frame}[c]
  \frametitle{Latent dimensions}
  % \framesubtitle{}

  \begin{center}
    \includegraphics[width=8cm]{img/3_buy_sell_labels_latent}
  \end{center}
\end{frame}

\begin{frame}[c]
  \frametitle{Semantic similarity graph (topological structure)}
  % \framesubtitle{}

  \begin{center}
    \begin{tabular}{@{}cc@{}}
    \includegraphics[width=55mm]{img/neighbourhood_coffee} &
    \visible<2->{\includegraphics[width=55mm]{img/neighbourhood_hand}}
  \end{tabular}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Context vectors \citep{Schuetze:98}}

  \gap
  \begin{columns}[c]
    \begin{column}{50mm}
      Distributional representation only at type level
      \begin{itemize}
      \item[\hand] What is the ``average'' meaning of \emph{mouse}? (computer \vs animal)
      \item[]
      \end{itemize}
      \pause
      \h{Context vector} approximates meaning of individual token
      \begin{itemize}
      \item \hh{bag-of-words} approach:
        centroid of all context words in the sentence
      \end{itemize}
    \end{column}
    \begin{column}{60mm}
      \includegraphics[width=60mm]{img/illustration_context_vectors_mouse}
    \end{column}
  \end{columns}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Quantitative evaluation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{Synonym identification and semantic similarity judgements}

\begin{frame}\frametitle{The TOEFL synonym task}

  \begin{itemize}
  \item The TOEFL dataset
    \begin{itemize}
    \item 80 items
    \item Target: \emph{levied}\\
      Candidates: \emph{believed, correlated, \alert<2->{imposed}, requested}
    \item<3-> Target \emph{fashion}\\
      Candidates: \emph{craze, fathom, \alert<4->{manner}, ration}
  \end{itemize}
    \item[]
  \item<5-> DSMs and TOEFL
  \begin{enumerate}
  \item take vectors of the target ($\Vector{t}$) and of the candidates
($\Vector{c}_1 \dots \Vector{c}_n$)
\item measure the distance between $\Vector{t}$ and $\Vector{c}_i$, with
$1 \leq i \leq n$ 
\item select $\Vector{c}_i$ with the shortest distance in space from $\Vector{t}$
\end{enumerate}
  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Humans \vs machines on the TOEFL task}

  \begin{itemize}
  \item Average foreign test taker: 64.5\%
  \item<2-> Macquarie University staff (Rapp 2004):
    \begin{itemize}
    \item Average of 5 non-natives: 86.75\%
    \item Average of 5 natives: \primary{97.75\%}
    \item[]
    \end{itemize}
  \item<3-> Distributional semantics
    \begin{itemize}
    \item Classic LSA \citep{Landauer:Dumais:97}: 64.4\%
    \item Pad처 and Lapata's (\citeyear{Pado:Lapata:07}) dependency-based model: 73.0\%
    \item Distributional memory \citep{Baroni:Lenci:10}: 76.9\%
    \item Rapp's (\citeyear{Rapp:04a}) SVD-based model, lemmatized BNC: 92.5\%
    \item \citet{Bullinaria:Levy:12} carry out aggressive parameter optimization: \primary{100.0\%}
    \end{itemize}
  \end{itemize}

\end{frame}


\begin{frame}
\frametitle{Semantic similarity judgments}

\begin{itemize}
\item \citet{Rubenstein:Goodenough:65} collected similarity ratings for 65 noun pairs from 51 subjects on a 0--4 scale
  \begin{center}
    \begin{tabular}{llr}
      $w_1$ & $w_2$ & avg.\ rating\\
      \toprule
      \emph{car} & \emph{automobile} & \secondary{3.9}\\
      \emph{food} &  \emph{fruit}  & \secondary{2.7}\\
      \emph{cord} & \emph{smile} & \secondary{0.0}\\
    \end{tabular}
  \end{center}
  \gap
\item<2-> DSMs \vs\ Rubenstein \& Goodenough
  \begin{enumerate}
  \item for each test pair $(w_1, w_2)$, take vectors $\Vector{w}_1$ and $\Vector{w}_2$
  \item measure the distance (e.g.\ cosine) between $\Vector{w}_1$ and $\Vector{w}_2$
  \item measure (Pearson) correlation between vector distances and R\&G average judgments \citep{Pado:Lapata:07}
  \end{enumerate}
\end{itemize}
\end{frame}  

\begin{frame}
  \frametitle{Semantic similarity judgments: example}
  \ungap[1]
  \begin{center}
    \includegraphics[width=9cm]{img/bnc_rg65}
  \end{center}
\end{frame}

\begin{frame}
\frametitle{Semantic similarity judgments: results}

Results on RG65 task:
\begin{itemize}
\item Pad처 and Lapata's (\citeyear{Pado:Lapata:07}) dependency-based model: 0.62
\item Dependency-based on Web corpus \citep{Herdagdelen:Erk:Baroni:09}
  \begin{itemize}
  \item without SVD reduction: 0.69
  \item with SVD reduction: 0.80
  \end{itemize}
\item Distributional memory \citep{Baroni:Lenci:10}: 0.82
\item Salient Semantic Analysis \citep{Hassan:Mihalcea:11}: 0.86
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Software and further information}

\begin{frame}
  \frametitle{Software packages}

  \begin{tabular}{>{\color{secondary}}ll>{\itshape}p{6cm}}
    \href{http://www.psych.ualberta.ca/~westburylab/downloads/HiDEx.download.html}{HiDEx} & C++ & re-implementation of the HAL model \citep{Lund:Burgess:96} \\
    \href{http://code.google.com/p/semanticvectors/}{SemanticVectors} & Java & scalable architecture based on random indexing representation \\
    \href{http://github.com/fozziethebeat/S-Space}{S-Space} & Java & complex object-oriented framework \\
    \href{http://maggie.lt.informatik.tu-darmstadt.de/jobimtext/}{JoBimText} & Java & UIMA / Hadoop framework \\
    \href{http://radimrehurek.com/gensim/}{Gensim} & Python & complex framework, focus on parallelization and out-of-core algorithms \\
    \href{http://clic.cimec.unitn.it/composes/toolkit/}{DISSECT} & Python & user-friendly, designed for research on compositional semantics \\
    \href{http://wordspace.r-forge.r-project.org/}{\color{primary}\texttt{wordspace}} & R & interactive research laboratory, but scales to real-life data sets
  \end{tabular}

  \vspace{1em}
  \hfill\light{\small click on package name to open Web page}
\end{frame}

\begin{frame}
  \frametitle{Recent conferences and workshops}
  % \framesubtitle{}

  \ungap[1]
  \begin{itemize}
  \item \primary{2007}: \href{http://clic.cimec.unitn.it/marco/beyond_words/}{CoSMo Workshop} (at Context '07)
  \item \primary{2008}: \href{http://wordspace.collocations.de/doku.php/workshop:esslli:start}{ESSLLI Lexical Semantics Workshop} \& \href{http://wordspace.collocations.de/doku.php/workshop:esslli:task}{Shared Task}, \href{http://linguistica.sns.it/RdL/2008.html}{Special Issue of the Italian Journal of Linguistics}
  \item \primary{2009}: \href{http://art.uniroma2.it/gems/}{GeMS Workshop} (EACL 2009), \href{http://www.let.rug.nl/disco2009/}{DiSCo Workshop} (CogSci 2009), \href{http://wordspace.collocations.de/doku.php/course:esslli2009:start}{ESSLLI Advanced Course on DSM}
  \item \primary{2010}: \href{http://art.uniroma2.it/gems010/}{2nd GeMS} (ACL 2010), \href{http://clic.cimec.unitn.it/roberto/ESSLLI10-dsm-workshop/}{ESSLLI Workshop on Compositionality and DSM}, \href{http://naaclhlt2010.isi.edu/tutorials/t4.html}{DSM Tutorial} (NAACL 2010), \href{http://journals.cambridge.org/action/displayIssue?iid=7911772}{Special Issue of JNLE on Distributional Lexical Semantics}
  \item \primary{2011}: \href{http://disco2011.fzi.de}{2nd DiSCo} (ACL 2011), \href{https://sites.google.com/site/geometricalmodels/}{3rd GeMS} (EMNLP 2011)
  \item \primary{2012}: \href{http://didas.org}{DiDaS} (at ICSC 2012)
  \item \primary{2013}: \href{https://sites.google.com/site/cvscworkshop/}{CVSC} (ACL 2013), \href{http://clic.cimec.unitn.it/roberto/IWCS-TFDS2013/}{TFDS} (IWCS 2013), \href{http://www.dagstuhl.de/en/program/calendar/semhp/?semnr=13462}{Dagstuhl}
  \item \primary{2014}: \href{https://sites.google.com/site/cvscworkshop2014/}{2nd CVSC} (at EACL 2014)
  \end{itemize}
  \hfill\light{\small click on Workshop name to open Web page}
  \addnote{CoSMo = \underline{Co}ntextual Information in \underline{S}emantic Space \underline{M}odels}%
  \addnote{ESSLLI = European Summer School in Logic, Language and Information}%
  \addnote{GeMS = \underline{Ge}ometrical \underline{M}odels of Natural Language \underline{S}emantics}%
  \addnote{DiSCo = \underline{Di}stributional \underline{S}emantics beyond Concrete \underline{Co}ncepts}%
  \addnote{JNLE = Journal of Natural Language Engineering}%
  \addnote{DiSCo 2 = \underline{Di}stributional \underline{S}emantics and \underline{Co}mpositionality}%
  \addnote{DiDaS = Workshop on \underline{Di}stributional \underline{Da}ta \underline{S}emantics}%
  \addnote{CVSC = \underline{C}ontinuous \underline{V}ector \underline{S}pace Models and their \underline{C}ompositionality}%
  \addnote{TFDS = Towards a Formal Distributional Semantics}%
\end{frame}


\begin{frame}
  \frametitle{Further information}
  % \framesubtitle{}

  \begin{itemize}
  \item Handouts \& other materials available from wordspace wiki at
    \begin{center}
      \secondary{\url{http://wordspace.collocations.de/}}
    \end{center}
    \begin{itemize}
    \item[\hand] based on joint work with Marco Baroni and Alessandro Lenci
    \end{itemize}
  \item Tutorial is open source (CC), and can be downloaded from
    \begin{center}\small
      \secondary{\url{http://r-forge.r-project.org/projects/wordspace/}}
    \end{center}
    \gap[.5]
  \item Review paper on distributional semantics:
    \begin{itemize}
    \item[] \small\nocite{Turney:Pantel:10}
      Turney, Peter~D. and Pantel, Patrick (2010).
      \primary{From frequency to meaning: Vector space models of semantics.}
      {\em Journal of Artificial Intelligence Research}, {\bf 37}, 141--188.%
      \nocite{Turney:Pantel:10}
    \end{itemize}
    \gap[.5]
  \item I should be working on textbook \primary{\emph{Distributional Semantics}} for \secondary{\emph{Synthesis Lectures on HLT}} (Morgan \& Claypool)
  \end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% References (if any)

\frame[allowframebreaks]{
  \frametitle{References}
  \bibliographystyle{natbib-stefan}
  \begin{scriptsize}
    \bibliography{dsm}
  \end{scriptsize}
}

\end{document}
