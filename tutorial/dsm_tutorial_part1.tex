%% \documentclass[handout,t]{beamer} % HANDOUT
%% \documentclass[handout,notes=show,t]{beamer} % NOTES
\documentclass[t]{beamer} % SLIDES
\usepackage{etex}

\usetheme{DSM}
\usepackage{beamer-tools-dsm}

\input{lib/math}  % basic mathematical notation
\input{lib/text}  % some useful macros for plain text
\input{lib/stat}  % notation for probability theory and statistics
\input{lib/vector}% convenience macros for vectors and matrices

\input{local/config} % local adjustments to configuration and macros

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Titlepage

\title[DSM Tutorial -- Part 1]{Distributional Semantic Models}
\subtitle{Part 1: Introduction}
\author[\textcopyright\ Evert/Lenci/Baroni/Lapesa]{%
  Stefan Evert\inst{1}\\
  {\footnotesize with  Alessandro Lenci\inst{2}, Marco Baroni\inst{3} and Gabriella Lapesa\inst{4}}}
\institute[CC-by-sa]{%
  \inst{1}Friedrich-Alexander-Universität Erlangen-Nürnberg, Germany\\
  \inst{2}University of Pisa, Italy\\
  \inst{3}University of Trento, Italy\\
  \inst{4}University of Stuttgart, Germany
}

\date[wordspace.collocations.de]{
  \href{http://wordspace.collocations.de/doku.php/course:start}{\primary{\small http://wordspace.collocations.de/doku.php/course:start}}\\
  \light{\tiny \dsmcopyright}}

\begin{document}

\showLogo
\frame{\titlepage}
\hideLogo

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Outline}
\frame{ 
  \frametitle{Outline}
  \tableofcontents
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The distributional hypothesis}

\begin{frame}[c]
  \frametitle{Meaning \& distribution}
  % \framesubtitle{}

  \begin{itemize}
  \item ``Die Bedeutung eines Wortes liegt in seinem Gebrauch.''\\
    \hfill --- Ludwig Wittgenstein
    \begin{itemize}
    \item<2->[\hand] \primary{meaning = use = distribution in language}
    \item[] 
    \end{itemize}
  \item ``You shall know a word by the company it keeps!''\\
    \hfill --- J.~R.\ \citet{Firth:57}
    \begin{itemize}
    \item<3->[\hand] \primary{distribution = collocations = habitual word combinations}
    \item[]
    \end{itemize}
  \item Distributional hypothesis: difference of meaning correlates with difference of distribution \citep[Zellig][]{Harris:54}
    \begin{itemize}
    \item<4->[\hand] \primary{semantic distance}
    \item[]
    \end{itemize}
  \item ``What people know when they say that they know a word is not how to recite its dictionary definition -- they know how to use it [\ldots] in everyday discourse.'' \citep{Miller:86}
  \end{itemize}
\end{frame}

\newcommand{\bardiwac}{\primary{\only<beamer:1-7| handout:1>{bardiwac}\only<beamer:8-| fhandout:0>{\makebox[\widthof{bardiwac}][c]{claret}}}}%
\begin{frame}
  \frametitle{What is the meaning of ``\textbf{bardiwac}''?}
  \framesubtitle{Can we infer meaning from usage?}

  \ungap
  \begin{itemize}
  \item<2-> He handed her her glass of \bardiwac{}.
  \item<3-> Beef dishes are made to complement the \bardiwac{}s.
  \item<4-> Nigel staggered to his feet, face flushed from too much \bardiwac{}.
  \item<5-> Malbec, one of the lesser-known \bardiwac{} grapes, responds well to Australia's sunshine.
  \item<6-> I dined off bread and cheese and this excellent \bardiwac{}.
  \item<7-> The drinks were delicious: blood-red \bardiwac{} as well as light, sweet Rhenish.
  \item[\hand]<8-> claret is a heavy red alcoholic beverage made from grapes
  \end{itemize}

  \gap
  \begin{footnotesize}
    \visible<8->{\light{All examples from British National Corpus (handpicked and slightly edited).}}
  \end{footnotesize}
\end{frame}

\begin{frame}[c]
  \frametitle{Word sketch of ``\textbf{cat}''}
  \framesubtitle{Can we infer meaning from collocations?}

  \centering\ungap[.5]
  \begin{tikzpicture}
    \node[anchor=north west] at (0,0) {\includegraphics[width=10cm]{img/SE_cat}} ;
    \node[anchor=north east] at (10,0) {\tiny\light{\url{https://the.sketchengine.co.uk/}}} ;
  \end{tikzpicture}
\end{frame}

{\newcommand{\hg}[1]{\scriptsize\textpmhg{#1}}
\newcommand<>{\colA}[1]{\purered#2{#1}}
\newcommand<>{\colB}[1]{\puregreen#2{\textbf#2{#1}}}
\begin{frame}<beamer:1-4| handout:1-4>
  \frametitle{A thought experiment: deciphering hieroglyphs}
  % \framesubtitle{}

  \ungap[1]
  \begin{center}
    \setlength{\arrayrulewidth}{1pt}
    \begin{tabular}{@{\rule{0mm}{1.4em} }lr*{6}{|c}|}
      && \hg{get} & \hg{sij} & \hg{ius} & \hg{hir} & \hg{iit} & \hg{kil} \\
      \hline
      \tikz[remember picture, inner sep=0pt]{\node(knife){\colB<beamer:2| handout:2>{(knife)}}} & \colB<beamer:2| handout:2>{\hg{naif}} & \colB<beamer:2| handout:2>{51} & \colB<beamer:2| handout:2>{20} & \colB<beamer:2| handout:2>{84} &  \colB<beamer:2| handout:2>{0} &  \colB<beamer:2| handout:2>{3} &  \colB<beamer:2| handout:2>{0} \\
      \hline
      \tikz[remember picture, inner sep=0pt]{\node(cat){\colB<beamer:4| handout:4>{(cat)}}}   & \colB<beamer:4| handout:4>{\hg{ket}}  &  \colB<beamer:4| handout:4>{52} & \colB<beamer:4| handout:4>{58} &  \colB<beamer:4| handout:4>{4} &  \colB<beamer:4| handout:4>{4} &  \colB<beamer:4| handout:4>{6} & \colB<beamer:4| handout:4>{26} \\
      \hline
      \tikz[remember picture, inner sep=0pt]{\node(dog){\h{???}}} & \h{\hg{dog}} & \colA{115} & \colA{83} & \colA{10} & \colA{42} & \colA{33} & \colA{17} \\
      \hline
      (boat)  & \hg{beut} &  59 & 39 & 23 &  4 &  0 &  0 \\
      \hline
      (cup)   & \hg{kap}  &  98 & 14 &  6 &  2 &  1 &  0 \\
      \hline
      \tikz[remember picture, inner sep=0pt]{\node(pig){\colB<beamer:3| handout:3>{(pig)}}}  & \colB<beamer:3| handout:3>{\hg{pigij}} &  \colB<beamer:3| handout:3>{12} & \colB<beamer:3| handout:3>{17} &  \colB<beamer:3| handout:3>{3} &  \colB<beamer:3| handout:3>{2} &  \colB<beamer:3| handout:3>{9} & \colB<beamer:3| handout:3>{27} \\
      \hline
      (banana) & \hg{nana} & 11 &  2 &  2 &  0 & 18 &  0 \\
      \hline
    \end{tabular}
    
    \gap[2]\large
    \begin{tikzpicture}[remember picture, overlay]
      \draw<beamer:2| handout:2>[<->,puregreen,very thick] (dog.west) to[out=160, in=200] (knife.west) ;
      \draw<beamer:3| handout:3>[<->,puregreen,very thick] (dog.west) to[out=210, in=150] (pig.west) ;
      \draw<beamer:4| handout:4>[<->,puregreen,very thick] (dog.west) to[out=150, in=210] (cat.west) ;
    \end{tikzpicture}
    \only<beamer:2| handout:2>{%
      sim(\colA{\hg{dog}}, \colB{\hg{naif}}) = 0.770 }%
    \only<beamer:3| handout:3>{%
      sim(\colA{\hg{dog}}, \colB{\hg{pigij}}) = 0.939 }%
    \only<beamer:4| handout:4>{%
      sim(\colA{\hg{dog}}, \colB{\hg{ket}}) = 0.961 }%
  \end{center}

  \addnote{Similarity scores are cosine similarities on sparse log-scaled frequencies ($\log (f+1)$).}%
\end{frame}

\begin{frame}
  \frametitle{English as seen by the computer \ldots}
  % \framesubtitle{}

  \begin{center}
    \ungap[1]
    \setlength{\arrayrulewidth}{1pt}
    \begin{tabular}{@{\rule{0mm}{1.2em} }l@{ }r*{6}{|c}|}
      && get & see & use & hear & eat & kill \\
      && \hg{get} & \hg{sij} & \hg{ius} & \hg{hir} & \hg{iit} & \hg{kil} \\
      \hline
      knife & \hg{naif} &  51 & 20 & 84 &  0 &  3 &  0 \\
      \hline
      cat   & \hg{ket}  &  52 & 58 &  4 &  4 &  6 & 26 \\
      \hline
      \h{dog} & \h{\hg{dog}} & \colA{115} & \colA{83} & \colA{10} & \colA{42} & \colA{33} & \colA{17} \\
      \hline
      boat  & \hg{beut} &  59 & 39 & 23 &  4 &  0 &  0 \\
      \hline
      cup   & \hg{kap}  &  98 & 14 &  6 &  2 &  1 &  0 \\
      \hline
      pig  & \hg{pigij} &  12 & 17 &  3 &  2 &  9 & 27 \\
      \hline
      banana & \hg{nana} & 11 &  2 &  2 &  0 & 18 &  0 \\
      \hline
    \end{tabular}
  \end{center}
  \hfill\light{\footnotesize verb-object counts from British National Corpus}
\end{frame}
}

\begin{frame}
  \frametitle{Geometric interpretation}
  % \framesubtitle{}

  \begin{columns}[T]
    \begin{column}{40mm}
      \begin{itemize}
      \item row vector \primary{$\vx_{\text{dog}}$} describes usage of word \emph{dog} in the corpus
      \item can be seen as coordinates of point in $n$-dimensional Euclidean space
       \end{itemize}
    \end{column}
    \begin{column}{75mm}      
      \gap[2]
      \begin{small}
        \setlength{\arrayrulewidth}{1pt}
        \begin{tabular}{r*{6}{|c}|}
          & get & see & use & hear & eat & kill \\
          \hline
          knife &  51 & 20 & 84 &  0 &  3 &  0 \\
          \hline
          cat  &  52 & 58 &  4 &  4 &  6 & 26 \\
          \hline
          \h{dog} & \primary{115} & \primary{83} & \primary{10} & \primary{42} & \primary{33} & \primary{17} \\
          \hline
          boat &  59 & 39 & 23 &  4 &  0 &  0 \\
          \hline
          cup  &  98 & 14 &  6 &  2 &  1 &  0 \\
          \hline
          pig  &  12 & 17 &  3 &  2 &  9 & 27 \\
          \hline
          banana & 11 &  2 &  2 &  0 & 18 &  0 \\
          \hline
        \end{tabular}
      \end{small}

      \begin{center}
        \h{co-occurrence matrix} $\mathbf{M}$
      \end{center}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Geometric interpretation}
  % \framesubtitle{}

  \begin{columns}[T]
    \begin{column}{40mm}
      \begin{itemize}
      \item row vector \primary{$\vx_{\text{dog}}$} describes usage of word \emph{dog} in the corpus
      \item can be seen as coordinates of point in $n$-dimensional Euclidean space
      \item illustrated for two dimensions:\\ \emph{get} and \emph{use}
      \item \primary{$\vx_{\text{dog}} = (115,10)$}
      \end{itemize}
    \end{column}
    \begin{column}{75mm}      
      \ungap[1]
      \includegraphics[width=75mm]{img/hieroglyph_2d_1}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Geometric interpretation}
  % \framesubtitle{}

  \begin{columns}[T]
    \begin{column}{40mm}
      \begin{itemize}
      \item similarity = spatial proximity (Euclidean dist.)
      \item location depends on frequency of noun ($f_{\text{dog}} \approx 2.7\cdot f_{\text{cat}}$)
      \end{itemize}
    \end{column}
    \begin{column}{75mm}      
      \ungap[1]
      \includegraphics[width=75mm]{img/hieroglyph_2d_2}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}<beamer:1| handout:0>
  \frametitle{Geometric interpretation}
  % \framesubtitle{}

  \begin{columns}[T]
    \begin{column}{40mm}
      \begin{itemize}
      \item vector can also be understood as arrow from origin
      \item direction more important than location
      \end{itemize}
    \end{column}
    \begin{column}{75mm}      
      \ungap[1]
      \includegraphics[width=75mm]{img/hieroglyph_2d_3}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Geometric interpretation}
  % \framesubtitle{}

  \begin{columns}[T]
    \begin{column}{40mm}
      \begin{itemize}
      \item vector can also be understood as arrow from origin
      \item direction more important than location
      \item<beamer:1-| handout:1-> use angle $\alpha$ as distance measure
      \item<beamer:2-| handout:2-> or normalise length $\norm{\vx_{\text{dog}}}$ of arrow
      \end{itemize}
    \end{column}
    \begin{column}{75mm}
      \ungap[1]
      \only<beamer:1| handout:1>{%
        \includegraphics[width=75mm]{img/hieroglyph_2d_4}}%
      \only<beamer:2| handout:2>{%
        \includegraphics[width=75mm]{img/hieroglyph_2d_5}}%
    \end{column}
  \end{columns}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Distributional semantic models}

\begin{frame}
  \frametitle{General definition of DSMs}
  % \framesubtitle{}

  \ungap
  \begin{block}{}
    A \h{distributional semantic model} (DSM) is a scaled and/or
    transformed co-occurrence matrix $\mathbf{M}$, such that each row $\vx$
    represents the distribution of a target term across contexts.
  \end{block}

  \begin{center}
    \begin{small}
      \setlength{\arrayrulewidth}{1pt}
      \begin{tabular}{r*{6}{|c}|}
        & get & see & use & hear & eat & kill \\
        \hline
        knife &  0.027 & -0.024 &  0.206 & -0.022 & -0.044 & -0.042 \\
        \hline
        cat   &  0.031 &  0.143 & -0.243 & -0.015 & -0.009 &  0.131 \\
        \hline
        \primary{dog}   & \primary{-0.026} &  \primary{0.021} & \primary{-0.212} &  \primary{0.064} &  \primary{0.013} &  \primary{0.014} \\
        \hline
        boat  & -0.022 &  0.009 & -0.044 & -0.040 & -0.074 & -0.042 \\
        \hline
        cup   & -0.014 & -0.173 & -0.249 & -0.099 & -0.119 & -0.042 \\
        \hline
        pig   & -0.069 &  0.094 & -0.158 &  0.000 &  0.094 &  0.265 \\
        \hline
        banana&  0.047 & -0.139 & -0.104 & -0.022 &  0.267 & -0.042 \\
        \hline
      \end{tabular}
    \end{small}
  \end{center}

  \hh{Term} = word, lemma, phrase, morpheme, word pair, \ldots
\end{frame}

\begin{frame}
  \frametitle{Building a distributional model}
  % \framesubtitle{}
  
  \footnotesize\centering
  \begin{tikzpicture}
    \node[diagram box=black] (corpus) at (0, 6.5) {pre-processed corpus with linguistic annotation} ;
    \visible<2->{
      \node[diagram box] (tt_terms) at (+3, 5) {define target \& feature terms} ;
      \draw[diagram arrow=counterpoint] (corpus) -- node[right=1.5em] {term-term matrix} (tt_terms) ;
    }
    \visible<3->{
      \node[diagram box] (tt_span) at (+3, 4) {type \& size of co-occurrence} ;
      \draw[diagram arrow] (tt_terms) -- (tt_span) ;
    }
    \visible<4->{
      \node[diagram box=black] (geom_prob) at (0, 3) {$\mathbf{M}$} ;
      \draw[diagram arrow] (tt_span) -- (geom_prob) ;
    }
    \visible<5->{
      \node[diagram box] (tc_terms) at (-3, 5) {define target terms} ; 
      \draw[diagram arrow=counterpoint] (corpus) -- node[left=1.5em] {term-context matrix} (tc_terms) ;
    }
    \visible<6->{
      \node[diagram box] (tc_context) at (-3, 4) {context tokens or types} ;
      \draw[diagram arrow] (tc_terms) -- (tc_context) ;
      \draw[diagram arrow] (tc_context) -- (geom_prob) ;
    }
    \visible<7->{
      \node[diagram ghost] (prob) at (2, 2) {} ;
      \draw[diagram arrow=counterpoint] (geom_prob) -- node[right=1.5em] (probanal) {probabilistic analysis} (prob) ;
      \node[below=1mm of probanal, text width=30mm, align=center, color=counterpoint] {embedding learned by\\ neural network} ;
    }
    \visible<8->{
      \node[diagram box] (scoring) at (-2, 2) {feature scaling} ;
      \draw[diagram arrow=counterpoint] (geom_prob) -- node[left=1.5em] {geometric analysis} (scoring) ;
    }
    \visible<9->{
      \node[diagram box] (metric) at (-2, 1) {similarity/distance measure + normalization} ;
      \draw[diagram arrow] (scoring) -- (metric) ;
    }
    \visible<10->{
      \node[diagram box] (dimred) at (-2, 0) {dimensionality reduction} ;
      \draw[diagram arrow] (metric) -- (dimred) ;
    }
  \end{tikzpicture}
\end{frame}

\begin{frame}
  \frametitle{Nearest neighbours}
  \framesubtitle{DSM based on verb-object relations from BNC, reduced to 100 dim.\ with SVD}
  % \framesubtitle{}

  Neighbours of \h{trousers} (cosine angle):
  \begin{itemize}\item[\hand]
    shirt (18.5), blouse (21.9), scarf (23.4), jeans (24.7), skirt (25.9),
    sock (26.2), shorts (26.3), jacket (27.8), glove (28.1), coat (28.8),
    cloak (28.9), hat (29.1), tunic (29.3), overcoat (29.4), pants (29.8),
    helmet (30.4), apron (30.5), robe (30.6), mask (30.8), tracksuit (31.0),
    jersey (31.6), shawl (31.6), \ldots
  \end{itemize}

  \gap\pause
  Neighbours of \h{rage} (cosine angle):
  \begin{itemize}\item[\hand]
    anger (28.5), fury (32.5), sadness (37.0), disgust (37.4), emotion (39.0),
    jealousy (40.0), grief (40.4), irritation (40.7), revulsion (40.7), scorn
    (40.7), panic (40.8), bitterness (41.6), resentment (41.8), indignation
    (41.9), excitement (42.0), hatred (42.5), envy (42.8), disappointment
    (42.9), \ldots 
  \end{itemize}
  \addnote{Neighbours and neighbourhood plots from BNC verb-object DSM, reduced to 100 dimensions by SVD.}%
\end{frame}

\begin{frame}[c]
  \frametitle{Nearest neighbours with similarity graph}
  % \framesubtitle{}

  \ungap[1]
  \begin{center}
    \only<beamer:0| handout:1>{%
      \begin{tabular}{@{}cc@{}}
        \includegraphics[width=55mm]{img/neighbourhood_trousers} 
        & \includegraphics[width=55mm]{img/neighbourhood_plant}
      \end{tabular}}%
    \only<beamer:1| handout:0>{%
       \includegraphics[width=75mm]{img/neighbourhood_trousers}}%
    \only<beamer:2| handout:0>{%
      \includegraphics[width=90mm, trim=0 50 0 50, clip]{img/neighbourhood_plant}}%
  \end{center}
\end{frame}

\begin{frame}[c]
  \frametitle{Semantic maps}
  % \framesubtitle{}

  \ungap[1]
  \begin{center}
    \includegraphics[width=100mm]{img/vobj_semantic_map}
  \end{center}
  \addnote{Roughly horizontal axis separates natural objects (left) from artifacts (right), or animate vs. inanimate There is a clear boundary between the two groups}%
  \addnote{Orthogonal axis separates moving things (bottom) from motionless ones (top).}%
\end{frame}

\begin{frame}[c]
  \frametitle{Clustering}
  % \framesubtitle{}

  \ungap[1]
  \begin{center}
    \only<beamer:1| handout:1>{\includegraphics[width=100mm]{img/vobj_clustering}}%
    \only<beamer:2| handout:0>{\includegraphics[width=100mm]{img/vobj_clustering_6}}%
  \end{center}
\end{frame}

\begin{frame}[c]
  \frametitle{Latent ``meaning'' dimensions}
  % \framesubtitle{}

  \ungap[1]
  \begin{center}
    \includegraphics[width=8cm]{img/3_buy_sell_labels_latent}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Word embeddings}

  \gap
  \begin{columns}[c]
    \begin{column}{50mm}
      DSM vector as sub-symbolic meaning representation
      \begin{itemize}
      \item feature vector for machine learning algorithm
      \item input for neural network
      \item[]
      \end{itemize}

      \onslide<2->
      \h{Context vectors} for word tokens \citep{Schuetze:98} 
      \begin{itemize}
      \item \hh{bag-of-words} approach:
        centroid of all context words in the sentence
      \item application to WSD
      \end{itemize}
    \end{column}
    \begin{column}{60mm}
      \onslide<3->
      \includegraphics[width=60mm]{img/illustration_context_vectors_mouse}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{An important distinction}

  \begin{itemize}
  \item<1-> \h{Distributional} \textbf{model}
    \begin{itemize}
    \item captures linguistic distribution of each word in the form of a high-dimensional numeric vector
    \item typically (but not necessarily) based on co-occurrence counts
    \item distributional hypothesis:\\
      distributional similarity/distance $\sim$ semantic similarity/distance
    \item[]
    \end{itemize}
  \item<2-> \h{Distributed} \textbf{representation}
    \begin{itemize}
    \item sub-symbolic representation of words as high-dimensional numeric vectors
    \item similarity of vectors usually (but not necessarily) corresponds to semantic similarity of the words
    \item hot topic: unsupervised neural \hh{word embeddings}
    \item[]
    \end{itemize}
  \end{itemize}
  
  \onslide<3->
  \hand\ Distributional model can be used as distributed representation

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Three famous examples}

\begin{frame}
  \frametitle{Latent Semantic Analysis \citep{Landauer:Dumais:97}}
  % \framesubtitle{}

  \begin{itemize}
  \item Corpus: 30,473 articles from Grolier's \emph{Academic American Encyclopedia} (4.6 million words in total)
    \begin{itemize}
    \item[\hand] articles were limited to first 2,000 characters
    \end{itemize}
  \item Word-article frequency matrix for 60,768 words
    \begin{itemize}
    \item row vector shows frequency of word in each article
    \end{itemize}
  \item Logarithmic frequencies scaled by word entropy
  \item Reduced to 300 dim.\ by singular value decomposition (SVD)
    \begin{itemize}
    \item borrowed from LSI \citep{Dumais:etc:88}
    \item[\hand] central claim: SVD reveals latent semantic features,\\
      not just a data reduction technique
    \end{itemize}
  \item Evaluated on TOEFL synonym test (80 items)
    \begin{itemize}
    \item LSA model achieved 64.4\% correct answers
    \item also simulation of learning rate based on TOEFL results
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Word Space \citep{Schuetze:92,Schuetze:93,Schuetze:98}}
  % \framesubtitle{}

  \begin{itemize}
  \item Corpus: $\approx 60$ million words of news messages
    \begin{itemize}
    \item from the \emph{New York Times} News Service
    \end{itemize}
  \item Word-word co-occurrence matrix
    \begin{itemize}
    \item 20,000 target words \& 2,000 context words as features
    \item row vector records how often each context word occurs close
      to the target word (co-occurrence)
    \item co-occurrence window: left/right 50 words \citep{Schuetze:98}\\
      or $\approx 1000$ characters \citep{Schuetze:92}
    \end{itemize}
  \item Rows weighted by inverse document frequency (tf.idf)
  \item Context vector = centroid of word vectors (bag-of-words)
    \begin{itemize}
    \item[\hand] goal: determine ``meaning'' of a context
    \end{itemize}
  \item Reduced to 100 SVD dimensions (mainly for efficiency)
  \item Evaluated on unsupervised word sense induction by clustering
    of context vectors (for an ambiguous word)
    \begin{itemize}
    \item induced word senses improve information retrieval performance
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{HAL \citep{Lund:Burgess:96}}
  % \framesubtitle{}

  \begin{itemize}
  \item HAL = Hyperspace Analogue to Language
  \item Corpus: 160 million words from newsgroup postings
  \item Word-word co-occurrence matrix
    \begin{itemize}
    \item same 70,000 words used as targets and features
    \item co-occurrence window of 1 -- 10 words
    \end{itemize}
  \item Separate counts for left and right co-occurrence
    \begin{itemize}
    \item i.e.\ the context is \emph{structured}
    \end{itemize}
  \item In later work, co-occurrences are weighted by (inverse) distance \citep{Li:Burgess:Lund:00}
    \begin{itemize}
      \item but no dimensionality reduction
    \end{itemize}
  \item Applications include construction of semantic vocabulary maps
    by multidimensional scaling to 2 dimensions
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{HAL \citep{Lund:Burgess:96}}
  % \framesubtitle{}

  \begin{center}
    \includegraphics[width=7cm]{img/LundBurgess1996_semantic_map}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Many parameters \ldots}
  % \framesubtitle{}

  \begin{itemize}
  \item Enormous range of DSM parameters and applications
  \item Examples showed three entirely different models, each tuned to
    its particular application
  \item[]
  \item<2->[\So] Need overview of DSM parameters \& understand their effects
    \begin{itemize}
    \item part 2: The parameters of a DSM
    \item part 3: Evaluating DSM representations
    \item part 4: Matrix algebra \& SVD
    \item part 5: Understanding distributional semantics
    \item[]
    \end{itemize}
  \item<3->[\So] Distributional semantics is an empirical science
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Getting practical}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Software and further information}

\begin{frame}
  \frametitle{Some applications in computational linguistics}
  % \framesubtitle{}

  \ungap[1.5]
  \begin{itemize}
  \item Query expansion in information retrieval \citep{Grefenstette:94}
  \item Unsupervised part-of-speech induction \citep{Schuetze:95}
  \item Word sense disambiguation \citep{Schuetze:98,Rapp:04b}
  \item Thesaurus compilation \citep{Lin:98b,Rapp:04a}
  \item Attachment disambiguation \citep*{Pantel:Lin:00}
  \item Probabilistic language models \citep{Bengio:etc:03b}
  \item Translation equivalents \citep{Sahlgren:Karlgren:05}
  \item Ontology \& wordnet expansion \citep{Pantel:etc:09}
  \item Language change \citep{Sagi:Kaufmann:Clark:09,Hamilton:etc:16}
  \item Multiword expressions \citep{Kiela:Clark:13}
  \item Analogies \citep{Turney:13,Gladkova:Drozd:Matsuoka:16}
  \item Sentiment analysis \citep{Rothe:Schuetze:16,Yu:etc:17}
  \item[\hand] Input representation for neural networks \& machine learning
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Recent workshops and tutorials}
  % \framesubtitle{}

  \ungap[1]
  \begin{itemize}
  \item \primary{2007}: \href{http://clic.cimec.unitn.it/marco/beyond_words/}{CoSMo Workshop} (at Context '07)
  \item \primary{2008}: \href{http://wordspace.collocations.de/doku.php/workshop:esslli:start}{ESSLLI Wshp} \& \href{http://wordspace.collocations.de/doku.php/workshop:esslli:task}{Shared Task}, \href{http://linguistica.sns.it/RdL/2008.html}{Italian J of Linguistics}
  \item \primary{2009}: \href{http://art.uniroma2.it/gems/}{GeMS Wshp} (EACL), \href{http://www.let.rug.nl/disco2009/}{DiSCo Wshp} (CogSci), \href{http://wordspace.collocations.de/doku.php/course:esslli2009:start}{ESSLLI}
  \item \primary{2010}: \href{http://art.uniroma2.it/gems010/}{2nd GeMS} (ACL), \href{http://clic.cimec.unitn.it/roberto/ESSLLI10-dsm-workshop/}{ESSLLI Wshp}, \href{http://naaclhlt2010.isi.edu/tutorials/t4.html}{Tutorial} (NAACL),\\
   \hide{2010:} \href{https://www.cambridge.org/core/journals/natural-language-engineering/issue/0C6E78AFDAB2AC263CDC331193B5E83A}{J Natural Language Engineering}
 \item \primary{2011}: \href{https://www.aclweb.org/anthology/W/W11/\#1300}{2nd DiSCo} (ACL), \href{https://sites.google.com/site/geometricalmodels/}{3rd GeMS} (EMNLP)
   %% DiSCo website seems offline: http://disco2011.fzi.de
  \item \primary{2012}: \href{http://didas.org}{DiDaS Wshp} (ICSC), \href{https://www.cl.cam.ac.uk/~ah433/esslli2012/coursecontent.html}{ESSLLI Course}
  \item \primary{2013}: \href{https://sites.google.com/site/cvscworkshop/}{CVSC Wshp} (ACL), \href{http://clic.cimec.unitn.it/roberto/IWCS-TFDS2013/}{TFDS Wshp} (IWCS), \href{http://www.dagstuhl.de/en/program/calendar/semhp/?semnr=13462}{Dagstuhl}
  \item \primary{2014}: \href{https://sites.google.com/site/cvscworkshop2014/}{2nd CVSC} (EACL), \href{https://sites.google.com/site/insightdistsemws/}{DSM Wshp} (Insight)
  \item \primary{2015}: \href{https://aclanthology.coli.uni-saarland.de/volumes/proceedings-of-the-1st-workshop-on-vector-space-modeling-for-natural-language-processing}{VSM4NLP} (NAACL), \href{https://staff.fnwi.uva.nl/w.zuidema/teaching/compositional-and-vectorial-semantics-esslli-2015/}{ESSLLI Course}, \href{https://hal.archives-ouvertes.fr/hal-01259695/}{TAL Journal}
  \item \primary{2016}: \href{http://esslli2016.unibz.it/?page_id=256}{DSALT Wshp} (ESSLLI), \href{http://coling2016.anlp.jp/tutorials/T1/}{Tutorial} (COLING), \href{https://sites.google.com/site/konvens2016jobimtexttutorial/}{Tutorial}\\
    \hide{2016:} (Konvens), \href{http://esslli2016.unibz.it/?page_id=242}{ESSLLI Course}, \href{http://www.aclweb.org/anthology/J/J16/\#4000}{Computational Linguistics}
  \item \primary{2017}: \href{https://www.irit.fr/esslli2017/courses/34}{ESSLLI Course}
  \item \primary{2018}: \href{http://text-machine.cs.uml.edu/lrec2018_t4/}{Tutorial} (LREC), \href{http://wordspace.collocations.de/doku.php/course:esslli2018:start}{ESSLLI Course\textsubscript{1}} \& \href{http://esslli2018.folli.info/word-vector-space-specialisation/}{Course\textsubscript{2}}
  \end{itemize}
  \hfill\light{\small click on Workshop name to open Web page}
  \addnote{CoSMo = \underline{Co}ntextual Information in \underline{S}emantic Space \underline{M}odels}%
  \addnote{ESSLLI = European Summer School in Logic, Language and Information}%
  \addnote{GeMS = \underline{Ge}ometrical \underline{M}odels of Natural Language \underline{S}emantics}%
  \addnote{DiSCo = \underline{Di}stributional \underline{S}emantics beyond Concrete \underline{Co}ncepts}%
  \addnote{JNLE = Journal of Natural Language Engineering}%
  \addnote{DiSCo 2 = \underline{Di}stributional \underline{S}emantics and \underline{Co}mpositionality}%
  \addnote{DiDaS = Workshop on \underline{Di}stributional \underline{Da}ta \underline{S}emantics}%
  \addnote{CVSC = \underline{C}ontinuous \underline{V}ector \underline{S}pace Models and their \underline{C}ompositionality}%
  \addnote{TFDS = Towards a Formal Distributional Semantics}%
\end{frame}

\begin{frame}
  \frametitle{Software packages}

  \begin{tabular}{>{\color{secondary}}ll>{\itshape}p{6cm}}
    \href{http://infomap-nlp.sourceforge.net/}{Infomap NLP} & C & classical LSA-style DSM \\
    \href{http://www.psych.ualberta.ca/~westburylab/downloads/HiDEx.download.html}{HiDEx} & C++ & re-implementation of the HAL model \citep{Lund:Burgess:96} \\
    \href{http://code.google.com/p/semanticvectors/}{SemanticVectors} & Java & scalable architecture based on random indexing representation \\
    \href{http://github.com/fozziethebeat/S-Space}{S-Space} & Java & complex object-oriented framework \\
    \href{http://ltmaggie.informatik.uni-hamburg.de/jobimtext/}{JoBimText} & Java & UIMA / Hadoop framework \\
    \href{http://radimrehurek.com/gensim/}{Gensim} & Python & complex framework, focus on parallelization and out-of-core algorithms \\
    \href{http://vecto.space/}{Vecto} & Python & framework for count \& predict models \\
    \href{http://clic.cimec.unitn.it/composes/toolkit/}{DISSECT} & Python & user-friendly, designed for research on compositional semantics \\
    \href{http://wordspace.r-forge.r-project.org/}{\color{primary}\texttt{wordspace}} & R & interactive research laboratory, but scales to real-life data sets
  \end{tabular}

  \gap[0.5]
  \hfill\light{\small click on package name to open Web page}
\end{frame}

\begin{frame}
  \frametitle{Further information}
  % \framesubtitle{}

  \begin{itemize}
  \item Handouts \& other materials available from wordspace wiki at
    \begin{center}
      \secondary{\url{http://wordspace.collocations.de/}}
    \end{center}
    \begin{itemize}
    \item[\hand] based on joint work with Marco Baroni and Alessandro Lenci
    \end{itemize}
  \item Tutorial is open source (CC), and can be downloaded from
    \begin{center}\small
      \secondary{\url{http://r-forge.r-project.org/projects/wordspace/}}
    \end{center}
    \gap[.5]
  \item Review paper on distributional semantics:
    \begin{itemize}
    \item[] \small\nocite{Turney:Pantel:10}
      Turney, Peter~D. and Pantel, Patrick (2010).
      \primary{From frequency to meaning: Vector space models of semantics.}
      {\em Journal of Artificial Intelligence Research}, {\bf 37}, 141--188.%
      \nocite{Turney:Pantel:10}
    \end{itemize}
    \gap[.5]
  \item I should be working on textbook \primary{\emph{Distributional Semantics}} for \secondary{\emph{Synthesis Lectures on HLT}} (Morgan \& Claypool)
  \end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{R as a (toy) laboratory}

\begin{frame}
  \frametitle{Prepare to get your hands dirty \ldots}
  
  \begin{itemize}
  \item We will use the statistical programming environment \href{http://www.r-project.org/}{\hh{R}} as a toy laboratory in this tutorial
    \begin{itemize}
    \item[\hand] but one that scales to real-life applications
    \item[]
    \end{itemize}
  \end{itemize}

  Software installation
  \begin{itemize}
  \item \hh{R} version 3.5 or newer from \url{http://www.r-project.org/}
  \item RStudio from \url{http://www.rstudio.com/}
  \item R packages from CRAN (through RStudio menu):
    \hh{\texttt{sparsesvd}}, \hh{\texttt{wordspace}} (optional: \secondary{\texttt{tm}}, \secondary{\texttt{quanteda}}, \secondary{\texttt{Rtsne}})
    \begin{itemize}
    \item if you are attending a course, you may also be asked to install the \hh{\texttt{wordspaceEval}} package with some non-public data sets
    \end{itemize}
  \item Get data sets, precompiled DSMs and \secondary{\texttt{wordspaceEval}} from\\ \primary{\href{http://wordspace.collocations.de/doku.php/course:material}{http://wordspace.collocations.de/doku.php/course:material}}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{First steps in R}

Start each session by loading the wordspace package.
\begin{Rcode}
> library(wordspace)
\end{Rcode}


The package includes various example data sets, some of which should look familiar to you.
\begin{Rcode}
> DSM_HieroglyphsMatrix\begin{Rout}
       get see use hear eat kill
knife   51  20  84    0   3    0
cat     52  58   4    4   6   26
dog    115  83  10   42  33   17
boat    59  39  23    4   0    0
cup     98  14   6    2   1    0
pig     12  17   3    2   9   27
banana  11   2   2    0  18    0
\end{Rout}
\end{Rcode}

\end{frame}

\begin{frame}[fragile]
  \frametitle{Term-term matrix}
  % \framesubtitle{}

  \h{Term-term matrix} records co-occurrence frequencies with feature terms for each target term 
\begin{Rcode}
> DSM_TermTermMatrix
\end{Rcode}

  \gap[2]
  \begin{center}
  \begin{small}
    \setlength{\arrayrulewidth}{1pt}
    \begin{tabular}[c]{r|c@{$\;$}*{6}{|@{$\;$}c@{$\;$}}|}
      \multicolumn{1}{c}{}
      & \multicolumn{1}{c}{\rotLabel{breed}}
      & \multicolumn{1}{c}{\rotLabel{tail}}
      & \multicolumn{1}{c}{\rotLabel{feed}}
      & \multicolumn{1}{c}{\rotLabel{kill}}
      & \multicolumn{1}{c}{\rotLabel{important}}
      & \multicolumn{1}{c}{\rotLabel{explain}}
      & \multicolumn{1}{c}{\rotLabel{likely}} \\
      \cline{2-8}
      cat     &  83 &  17 &   7 &  37 &  -- &   1 &  -- \\
      \cline{2-8}
      dog     & 561 &  13 &  30 &  60 &   1 &   2 &   4 \\
      \cline{2-8}
      animal  &  42 &  10 & 109 & 134 &  13 &   5 &   5 \\
      \cline{2-8}
      time    &  19 &   9 &  29 & 117 &  81 &  34 & 109 \\
      \cline{2-8}
      reason  &   1 &  -- &   2 &  14 &  68 & 140 &  47 \\
      \cline{2-8}
      cause   &  -- &   1 &  -- &   4 &  55 &  34 &  55 \\
      \cline{2-8}
      effect  &  -- &  -- &   1 &   6 &  60 &  35 &  17 \\
      \cline{2-8}
    \end{tabular}
  \end{small}
  \end{center}
\end{frame}


\begin{frame}[fragile]
  \frametitle{Term-context matrix}
  % \framesubtitle{}

  \h{Term-context matrix} records frequency of term in each individual context (e.g.\ sentence, document, Web page, encyclopaedia article)
\begin{Rcode}
> DSM_TermContextMatrix
\end{Rcode}
  
  \gap[2]
  \begin{center}
  \begin{small}
    \setlength{\arrayrulewidth}{1pt}
    \begin{tabular}[c]{r*{7}{|c}|}
      \multicolumn{1}{c}{}
      & \multicolumn{1}{c}{\rotLabel{Felidae}}
      & \multicolumn{1}{c}{\rotLabel{Pet}}
      & \multicolumn{1}{c}{\rotLabel{Feral}}
      & \multicolumn{1}{c}{\rotLabel{Bloat}}
      & \multicolumn{1}{c}{\rotLabel{Philosophy}}
      & \multicolumn{1}{c}{\rotLabel{Kant}}
      & \multicolumn{1}{c}{\rotLabel{Back pain}} \\
      \cline{2-8}
      cat      &      10 &  10 &     7 &    -- &         -- &   -- &        -- \\
      \cline{2-8}
      dog      &      -- &  10 &     4 &    11 &         -- &   -- &        -- \\
      \cline{2-8}
      animal   &       2 &  15 &    10 &     2 &         -- &   -- &        -- \\
      \cline{2-8}
      time     &       1 &  -- &    -- &    -- &          2 &    1 &        -- \\
      \cline{2-8}
      reason   &      -- &   1 &    -- &    -- &          1 &    4 &         1 \\
      \cline{2-8}
      cause    &      -- &  -- &    -- &     2 &          1 &    2 &         6 \\
      \cline{2-8}
      effect   &      -- &  -- &    -- &     1 &         -- &    1 &        -- \\
      \cline{2-8}
    \end{tabular}
  \end{small}
  \end{center}

\end{frame}

\begin{frame}[fragile]
  \frametitle{Some basic operations on a DSM matrix}

\begin{Rcode}
\REM{apply log-transformation to de-skew co-occurrence frequencies}
> M <- log2(DSM_HieroglyphsMatrix + 1) \REM{see part 2}
> round(M, 3)

\REM{compute semantic distance (cosine similarity)}
> pair.distances("dog", "cat", M, convert=FALSE)\begin{Rout}
  dog/cat 
0.9610952\end{Rout}

\REM{find nearest neighbours}
> nearest.neighbours(M, "dog", n=3)\begin{Rout}
     cat      pig      cup 
16.03458 20.08826 31.77784\end{Rout}

> plot(nearest.neighbours(M, "dog", n=3, dist.matrix=TRUE))
\end{Rcode}

\end{frame}

\begin{frame}[c]
  \begin{center}
    \huge
    \h{Thank} \hh{you!}
  \end{center}

  \gap[2]
  Until part 2, you can explore some DSM similarity networks online:
  \begin{itemize}
  \item \secondary{\href{https://corpora.linguistik.uni-erlangen.de/shiny/wordspace/}{https://corpora.linguistik.uni-erlangen.de/shiny/wordspace/}}
  \item built in R with \texttt{wordspace} and \texttt{shiny}
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% References (if any)

\frame[allowframebreaks]{
  \frametitle{References}
  \bibliographystyle{natbib-stefan}
  \begin{scriptsize}
    \bibliography{dsm}
  \end{scriptsize}
}

\end{document}
