\begin{frame}
  \frametitle{Scaling up to the real world}
  %% \framesubtitle{}

  \begin{itemize}
  \item So far, we have worked on small \hh{toy models}
    \begin{itemize}
    \item DSM matrix restricted to 2,000 -- 5,000 rows and columns
    \item small corpora (or dependency sets) can be processed within
      \textbf{R}
    \end{itemize}
    \pause
  \item Now we need to scale up to \h{real world} data sets
    \begin{itemize}
    \item for most statistical models, more data are better data!
    \item cf.\ success of Google-based NLP techniques (even if simplistic)
    \end{itemize}
    \pause
  \item Example 1: window-based DSM on BNC content words
    \begin{itemize}
    \item 83,926 lemma types with $f\geq 10$
    \item term-term matrix with 83,926 $\cdot$ 83,926 = 7 billion entries
    \item standard representation requires 56 GB of RAM (8-byte floats)%
      \pause
    \item only 22.1 million non-zero entries ($= 0.32\%$)
    \end{itemize}
    \pause
  \item Example 2: Google Web 1T 5-grams (1 trillion words)
    \begin{itemize}
    \item more than 1 million word types with $f\geq 2500$
    \item term-term matrix with 1 trillion entries requires 8 TB RAM
    \item only 400 million non-zero entries ($= 0.04\%$)
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Handling large data sets: three approaches}
  %% \framesubtitle{}

  \begin{enumerate}
  \item Sparse matrix representation
    \begin{itemize}
    \item full DSM matrix does not fit into memory
    \item but much smaller number of non-zero entries can be handled
    \item[] \pause
    \end{itemize}
  \item Feature selection
    \begin{itemize}
    \item reduce DSM matrix to subset of columns (usu.\ 2,000 -- 10,000)
    \item select most frequent, salient, discriminative, \ldots\ features
    \item[] \pause
    \end{itemize}
  \item Dimensionality reduction
    \begin{itemize}
    \item also reduces number of columns, but maps vectors to subspace
    \item singular value decomposition (usu.\ ca.\ 300 dimensions)
    \item random indexing (2,000 or more dimensions)
    \item performed with external tools \so \textbf{R} can handle reduced matrix
    \end{itemize}
  \end{enumerate}
\end{frame}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../../workspace"
%%% End: 
